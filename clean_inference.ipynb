{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d0e0b78",
   "metadata": {},
   "source": [
    "# ======= inference =======\n",
    "# ======= 토큰화 준비 ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4c75da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import os\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from tokenizers import(\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from tokenizers import processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bfb9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_essays.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa0d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c330d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWERCASE = False\n",
    "VOCAB_SIZE = 30522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253acca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer 초기화\n",
    "raw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Normalizer 설정\n",
    "raw_tokenizer.normalizer = normalizers.Sequence([normalizers.NFC()] + [normalizers.Lowercase()] if LOWERCASE else [])\n",
    "\n",
    "# Pre-tokenizer 설정: Byte-Level pre-tokenizer를 사용\n",
    "# \"Ġ\" 문자 제거 하고 싶다면 -> add_prefix_space=False 추가\n",
    "raw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "\n",
    "# BPE Trainer 설정 및 스페셜 토큰 추가\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)\n",
    "\n",
    "# 데이터셋 로드 및 토크나이저 훈련 (훈련 셋 : df_train_essays_final[['text']])\n",
    "dataset = Dataset.from_pandas(test[['text']])\n",
    "def train_corp_iter(): \n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000][\"text\"]\n",
    "raw_tokenizer.train_from_iterator(train_corp_iter(), trainer=trainer)\n",
    "\n",
    "# PreTrainedTokenizerFast로 래핑\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=raw_tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8820d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids = []\n",
    "test_attention_masks = []\n",
    "\n",
    "for text in tqdm(test['text'].tolist(), desc=\"토큰화 진행중\"):\n",
    "    encoded = tokenizer.encode_plus(text, \n",
    "                                    add_special_tokens=True, \n",
    "                                    max_length=256, \n",
    "                                    padding='max_length', \n",
    "                                    truncation=True, \n",
    "                                    return_attention_mask=True, \n",
    "                                    return_tensors='np')\n",
    "\n",
    "    test_input_ids.append(encoded['input_ids'][0])\n",
    "    test_attention_masks.append(encoded['attention_mask'][0])\n",
    "    # 토큰화된 데이터를 pandas DataFrame으로 변환\n",
    "test_inputs = pd.DataFrame({\n",
    "    'input_ids': test_input_ids,\n",
    "    'attention_mask': test_attention_masks\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cd0f87",
   "metadata": {},
   "source": [
    "## ============== 예측 진행 ================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2408231",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"roberta-base-finetuned_v5/checkpoint-2300\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e919b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls -l /kaggle/input/detect-llm-models/distilroberta-finetuned_v5          # \"모델이름\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9781124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_checkpoint = \"/kaggle/input/detect-llm-models/roberta-base-finetuned_v5/checkpoint-49654\"        #\"모델이름\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c3a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# def preprocess_function(examples):\n",
    "#     return tokenizer(examples['text'], max_length = 512 , padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d27980",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Move your model and data to the GPU\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9660a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "test_dataset = Dataset.from_pandas(test_inputs)\n",
    "# test_ds_enc = test_ds.map(preprocess_function, batched=True)\n",
    "test_preds = trainer.predict(test_dataset)\n",
    "logits = test_preds.predictions\n",
    "probs = (np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True))[:,0]\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = test['id']\n",
    "sub['generated'] = probs\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb7c630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b856553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c01a129",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.10.0-py3.8-cuda11.3",
   "language": "python",
   "name": "torch1.10.0-py3.8-cuda11.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
